{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "def decision_tree(df):\n",
    "    X=df[df.columns[:-1]]\n",
    "    y=df[df.columns[-1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    return f1_score(y_test, clf.predict(X_test), average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def random_forest(df):\n",
    "    X=final_dataset[final_dataset.columns[:-1]]\n",
    "    y=final_dataset[final_dataset.columns[-1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    #hyperparameter-tuning\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 100)]\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(1, 20, num = 100)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'bootstrap': bootstrap}\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_cv = RandomizedSearchCV(estimator = rf, param_distributions = grid, cv = 5, random_state=42, scoring='f1_macro')\n",
    "    # Fit the random search model\n",
    "    rf_cv.fit(X_train, y_train)\n",
    "    return f1_score(y_test, rf_cv.best_estimator_.predict(X_test), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def kNN(df):\n",
    "    X=df[df.columns[:-1]]\n",
    "    y=df[df.columns[-1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    #hyperparameter-tuning\n",
    "    knn=KNeighborsClassifier()\n",
    "    #Create grid\n",
    "    param_grid = {\"n_neighbors\": np.arange(1, 25)}\n",
    "    knn_cv = GridSearchCV(knn, param_grid, cv=5, scoring='f1_macro')\n",
    "    # Fit the random search model\n",
    "    knn_cv.fit(X_train,y_train)\n",
    "    return f1_score(y_test, knn_cv.best_estimator_.predict(X_test), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def gradient_boosting(df):\n",
    "    X=df[df.columns[:-1]]\n",
    "    y=df[df.columns[-1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    #hyperparameter-tuning\n",
    "    n_estimators = np.arange(1,100)\n",
    "    learning_rate = np.linspace(start=1e-1, stop=1, num=10)\n",
    "    max_depth = np.arange(1,50)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate}\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf_cv = RandomizedSearchCV(estimator = clf, param_distributions = grid, cv = 5, random_state=42, scoring='f1_macro')\n",
    "    \n",
    "    # Fit the random search model\n",
    "    clf_cv.fit(X_train, y_train)\n",
    "    return f1_score(y_test, clf_cv.best_estimator_.predict(X_test), average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Neural Networks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(df):\n",
    "    X=df[df.columns[:-1]]\n",
    "    y=df[df.columns[-1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = [\n",
    "    {'solver': ['adam', 'sgd'],\n",
    "     'learning_rate_init': [0.0001],\n",
    "     'max_iter': [10000],\n",
    "     'hidden_layer_sizes': [(50, 40), (40, 40), (30, 30), (20, 20)],\n",
    "     'activation': ['logistic', 'tanh', 'relu'],\n",
    "     'alpha': [0.0001, 0.001, 0.005],\n",
    "     'early_stopping': [True, False]\n",
    "     }\n",
    "]\n",
    "    clf = MLPClassifier()\n",
    "    clf_cv = RandomizedSearchCV(estimator = clf, param_distributions = grid, cv = 5, random_state=42, scoring='f1_macro')\n",
    "\n",
    "    # Fit the random search model\n",
    "    clf_cv.fit(X_train, y_train)\n",
    "    return f1_score(y_test, clf_cv.best_estimator_.predict(X_test), average='macro')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 Dummy Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "def dummy_classifier(df):\n",
    "    X=df[df.columns[:-1]]\n",
    "    y=df[df.columns[-1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = DummyClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return f1_score(y_test, clf.predict(X_test), average='macro')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Creating dataset with different functions and test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_column_switch(df, column1, column2):\n",
    "    i = list(df.columns)\n",
    "    a, b = i.index(column1), i.index(column2)\n",
    "    i[b], i[a] = i[a], i[b]\n",
    "    df = df[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1.5\n",
    "b=3\n",
    "def function_log_exp(x,y):\n",
    "    return (np.exp(a*x))/((1/a)*np.log(y+1))\n",
    "def function_log_power(x,y):\n",
    "    return (np.power(x,b))/(np.log(y+1))\n",
    "def function_log(x,y):\n",
    "    return x/(np.log(y+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_dataset(df, function):\n",
    "    #compute value\n",
    "    df['value'] = function(df['F1'], df['time'])\n",
    "    df = df.drop(columns=['F1', 'time', 'consumption','TDP', 'country'])\n",
    "    #keep most green entry for each dataset\n",
    "    final_dataset = df.sort_values('value').drop_duplicates(df.columns[:-2],keep='last')\n",
    "    final_dataset = df_column_switch(final_dataset, 'algo', 'value')\n",
    "    final_dataset = final_dataset.drop(columns = ['value'])\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autoML(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df[df.columns[-1]], test_size=0.25)\n",
    "    automl = AutoML(eval_metric='f1', explain_level=1, top_models_to_improve=4, random_state=2, optuna_verbose=False)\n",
    "    automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Selecting best function for evaluating \"greenness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 ------h------------------------------------------------------------------ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "The least populated class in y has only 3 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-ed6d5c1385c9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0;31m#final_dataset[features]=(final_dataset[features]-final_dataset[features].min())/(final_dataset[features].max()-final_dataset[features].min())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlist_models\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m         \u001B[0mf1_scores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'clf'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinal_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mylabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"F1 Score\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxlabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Model Used\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-4aab5536fa5d>\u001B[0m in \u001B[0;36mgradient_boosting\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;31m# Fit the random search model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     \u001B[0mclf_cv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclf_cv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'macro'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    839\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    840\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 841\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    842\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    843\u001B[0m             \u001B[0;31m# multimetric is determined here because in the case of a callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36m_run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1631\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_run_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1632\u001B[0m         \u001B[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1633\u001B[0;31m         evaluate_candidates(ParameterSampler(\n\u001B[0m\u001B[1;32m   1634\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparam_distributions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1635\u001B[0m             random_state=self.random_state))\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mevaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    793\u001B[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001B[1;32m    794\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 795\u001B[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001B[0m\u001B[1;32m    796\u001B[0m                                                        \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    797\u001B[0m                                                        \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1044\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_original_iterator\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1045\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1046\u001B[0;31m             \u001B[0;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch_one_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1047\u001B[0m                 \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1048\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    859\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    860\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 861\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    862\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    863\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m_dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    777\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    778\u001B[0m             \u001B[0mjob_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 779\u001B[0;31m             \u001B[0mjob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_async\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    780\u001B[0m             \u001B[0;31m# A job can complete so quickly than its callback is\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    781\u001B[0m             \u001B[0;31m# called before we get here, causing self._jobs to\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mapply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mapply_async\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m         \u001B[0;34m\"\"\"Schedule a func to be run\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImmediateResult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m             \u001B[0mcallback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    570\u001B[0m         \u001B[0;31m# Don't delay the application, to avoid keeping the input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    571\u001B[0m         \u001B[0;31m# arguments in memory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 572\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0;31m# change the default number of processes to -1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m             return [func(*args, **kwargs)\n\u001B[0m\u001B[1;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0;31m# change the default number of processes to -1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m             return [func(*args, **kwargs)\n\u001B[0m\u001B[1;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    220\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mconfig_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 222\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001B[0m in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    596\u001B[0m             \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 598\u001B[0;31m             \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    599\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    503\u001B[0m         \u001B[0;31m# fit the boosting stages\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 504\u001B[0;31m         n_stages = self._fit_stages(\n\u001B[0m\u001B[1;32m    505\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mraw_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_rng\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_val\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    506\u001B[0m             sample_weight_val, begin_at_stage, monitor)\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001B[0m in \u001B[0;36m_fit_stages\u001B[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m             \u001B[0;31m# fit next stage of trees\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 561\u001B[0;31m             raw_predictions = self._fit_stage(\n\u001B[0m\u001B[1;32m    562\u001B[0m                 \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mraw_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    563\u001B[0m                 random_state, X_csc, X_csr)\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001B[0m in \u001B[0;36m_fit_stage\u001B[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[0m\n\u001B[1;32m    189\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal_y\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,\n\u001B[0m\u001B[1;32m    192\u001B[0m                                               sample_weight=sample_weight)\n\u001B[1;32m    193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_gb_losses.py\u001B[0m in \u001B[0;36mnegative_gradient\u001B[0;34m(self, y, raw_predictions, k, **kwargs)\u001B[0m\n\u001B[1;32m    718\u001B[0m         \"\"\"\n\u001B[1;32m    719\u001B[0m         return y - np.nan_to_num(np.exp(raw_predictions[:, k] -\n\u001B[0;32m--> 720\u001B[0;31m                                         logsumexp(raw_predictions, axis=1)))\n\u001B[0m\u001B[1;32m    721\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    722\u001B[0m     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('ml_dataset.csv')\n",
    "dataset_cleaned  = dataset[dataset.algo != 'Ensemble']\n",
    "dataset_cleaned = dataset_cleaned.drop(columns=['did'])\n",
    "dataset_cleaned['consumption']=dataset_cleaned['time']*dataset_cleaned['TDP']\n",
    "\n",
    "list_functions=[function_log_exp, function_log_power, function_log]\n",
    "list_models=[{\"name\":\"decision tree\", \"clf\":decision_tree}, {\"name\":\"kNN\", \"clf\":kNN}, {\"name\":\"random forest\", \"clf\":random_forest},{\"name\":\"gradient boosting\",  \"clf\":gradient_boosting}, {\"name\":\"neural network\", \"clf\":neural_network}, {\"name\":\"dummy\", \"clf\":dummy_classifier}]\n",
    "#list_models=[{\"name\":\"NN\", \"clf\":neural_network}]\n",
    "names=[x['name'] for x in list_models]\n",
    "average_f1=[]\n",
    "# with open('out.txt', 'w') as f:\n",
    "#     with redirect_stdout(f):\n",
    "for i,function in enumerate(list_functions):\n",
    "    # run_autoML(final_dataset)\n",
    "    f1_scores=[]\n",
    "    print(f'STEP {i} ------h------------------------------------------------------------------ ')\n",
    "    dataset_cleaned_copy=dataset_cleaned.copy()\n",
    "    final_dataset=compute_final_dataset(dataset_cleaned_copy, function)\n",
    "    \n",
    "    #drop na\n",
    "    final_dataset=final_dataset.dropna()\n",
    "    #min-max normalization\n",
    "    #final_dataset[features]=(final_dataset[features]-final_dataset[features].min())/(final_dataset[features].max()-final_dataset[features].min())\n",
    "    for model in list_models:\n",
    "        f1_scores.append(model['clf'](final_dataset))\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Model Used\")\n",
    "    plt.bar(names, f1_scores)\n",
    "    plt.show()\n",
    "    print(f'Max F1-score: {np.max(np.array(f1_scores))}')\n",
    "    average_f1.append(np.mean(np.array(f1_scores)))\n",
    "    \n",
    "print(f'FINAL')\n",
    "plt.ylabel(\"Average F1 Score\")\n",
    "plt.xlabel(\"Function group Used\")\n",
    "names=[f'Group {i}°' for i in range(len(list_functions))]\n",
    "plt.bar(names, average_f1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute features for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer,LabelEncoder\n",
    "\n",
    "def encoder(ya, yb):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(ya)\n",
    "    y_enc = le.transform(yb)\n",
    "    return y_enc\n",
    "\n",
    "def scaler(a,b):\n",
    "    s = StandardScaler().fit(a)\n",
    "    return s.transform(b)\n",
    "\n",
    "def normalizer(a,b):\n",
    "    s = Normalizer().fit(a)\n",
    "    return s.transform(b)\n",
    "\n",
    "\n",
    "def compute_max_corr(df):\n",
    "    y = encode_y(df[df.columns[-1]])\n",
    "    y = pd.Series(y)\n",
    "    corr = df[df.columns[:-1]].corrwith(y)\n",
    "    return np.max(np.absolute(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df):\n",
    "    label=df.columns[-1]\n",
    "    features=df.columns[:-1]\n",
    "    computed_features=[]\n",
    "\n",
    "    MajorityClassSize=df.groupby(by=label).size().max()\n",
    "    computed_features.append(MajorityClassSize)\n",
    "\n",
    "    MaxNominalAttDistinctValues=df[features].select_dtypes(exclude=[np.number]).nunique().values.max()#must check this\n",
    "    computed_features.append(MaxNominalAttDistinctValues)\n",
    "\n",
    "    MinorityClassSize=df.groupby(by=label).size().min()\n",
    "    computed_features.append(MinorityClassSize)\n",
    "\n",
    "    NumberOfClasses=df[label].unique().shape[0]\n",
    "    computed_features.append(NumberOfClasses)\n",
    "\n",
    "    NumberOfFeatures=df.shape[1]-1\n",
    "    computed_features.append(NumberOfFeatures)\n",
    "\n",
    "    NumberOfInstances=df.shape[0]\n",
    "    computed_features.append(NumberOfInstances)\n",
    "\n",
    "    NumberOfInstancesWithMissingValues=df.isnull().any(axis=1).sum()\n",
    "    computed_features.append(NumberOfInstancesWithMissingValues)\n",
    "\n",
    "    NumberOfMissingValues=df.isnull().all(axis=1).sum()\n",
    "    computed_features.append(NumberOfMissingValues)\n",
    "\n",
    "    NumberOfNumericFeatures=df[features].select_dtypes([np.number]).shape[1]\n",
    "    computed_features.append(NumberOfNumericFeatures)\n",
    "\n",
    "    NumberOfSymbolicFeatures=df[features].select_dtypes(exclude=[np.number]).shape[1]#must check this\n",
    "    computed_features.append(NumberOfSymbolicFeatures)\n",
    "\n",
    "    max_corr=compute_max_corr(df)\n",
    "    computed_features.append(max_corr)\n",
    "\n",
    "    return np.array(computed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}